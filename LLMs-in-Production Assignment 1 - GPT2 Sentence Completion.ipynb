{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayinc/LLMs4Production/blob/main/LLM_Maven_v2_module_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I7DweFkH0xG"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Create a placeholder for model\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        "\n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5snK0qDwH6pP",
        "outputId": "cda9e874-403f-4e76-e3e7-6df268242371"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict(model[\"the\",\"price\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-prAi17J5Fa",
        "outputId": "f00873ec-9ba4-49c3-facd-598ba73ba510"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'yesterday': 0.004651162790697674,\n",
              " 'of': 0.3209302325581395,\n",
              " 'it': 0.05581395348837209,\n",
              " 'effect': 0.004651162790697674,\n",
              " 'cut': 0.009302325581395349,\n",
              " 'for': 0.05116279069767442,\n",
              " 'paid': 0.013953488372093023,\n",
              " 'to': 0.05581395348837209,\n",
              " 'increases': 0.013953488372093023,\n",
              " 'used': 0.004651162790697674,\n",
              " 'climate': 0.004651162790697674,\n",
              " '.': 0.023255813953488372,\n",
              " 'cuts': 0.009302325581395349,\n",
              " 'reductions': 0.004651162790697674,\n",
              " 'limit': 0.004651162790697674,\n",
              " 'now': 0.004651162790697674,\n",
              " 'moved': 0.004651162790697674,\n",
              " 'per': 0.013953488372093023,\n",
              " 'adjustments': 0.004651162790697674,\n",
              " '(': 0.009302325581395349,\n",
              " 'slumped': 0.004651162790697674,\n",
              " 'is': 0.018604651162790697,\n",
              " 'move': 0.004651162790697674,\n",
              " 'evolution': 0.004651162790697674,\n",
              " 'differentials': 0.009302325581395349,\n",
              " 'went': 0.004651162790697674,\n",
              " 'the': 0.013953488372093023,\n",
              " 'factor': 0.004651162790697674,\n",
              " 'Royal': 0.004651162790697674,\n",
              " ',': 0.018604651162790697,\n",
              " 'again': 0.004651162790697674,\n",
              " 'changes': 0.004651162790697674,\n",
              " 'holds': 0.004651162790697674,\n",
              " 'has': 0.009302325581395349,\n",
              " 'fall': 0.004651162790697674,\n",
              " '-': 0.004651162790697674,\n",
              " 'from': 0.004651162790697674,\n",
              " 'base': 0.004651162790697674,\n",
              " 'on': 0.004651162790697674,\n",
              " 'review': 0.004651162790697674,\n",
              " 'while': 0.004651162790697674,\n",
              " 'collapse': 0.004651162790697674,\n",
              " 'being': 0.004651162790697674,\n",
              " 'at': 0.023255813953488372,\n",
              " 'outlook': 0.004651162790697674,\n",
              " 'rises': 0.004651162790697674,\n",
              " 'drop': 0.004651162790697674,\n",
              " 'guaranteed': 0.004651162790697674,\n",
              " ',\"': 0.004651162790697674,\n",
              " 'stayed': 0.009302325581395349,\n",
              " 'structure': 0.004651162790697674,\n",
              " 'and': 0.004651162790697674,\n",
              " 'could': 0.004651162790697674,\n",
              " 'related': 0.004651162790697674,\n",
              " 'hike': 0.004651162790697674,\n",
              " 'we': 0.004651162790697674,\n",
              " 'adjustment': 0.023255813953488372,\n",
              " 'policy': 0.004651162790697674,\n",
              " 'was': 0.009302325581395349,\n",
              " 'revision': 0.004651162790697674,\n",
              " 'freeze': 0.009302325581395349,\n",
              " 'led': 0.004651162790697674,\n",
              " 'action': 0.004651162790697674,\n",
              " 'zone': 0.004651162790697674,\n",
              " 'slump': 0.004651162790697674,\n",
              " 'had': 0.004651162790697674,\n",
              " 'difference': 0.004651162790697674,\n",
              " 'in': 0.004651162790697674,\n",
              " 'raise': 0.004651162790697674,\n",
              " 'increase': 0.009302325581395349,\n",
              " 'will': 0.013953488372093023,\n",
              " 'support': 0.004651162790697674,\n",
              " 'gap': 0.004651162790697674,\n",
              " 'would': 0.009302325581395349,\n",
              " 'projected': 0.004651162790697674,\n",
              " 'approached': 0.004651162790697674,\n",
              " 'instability': 0.004651162790697674}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        "\n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold\n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in model[tuple(text[-2:])].keys():\n",
        "      accumulator += model[tuple(text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(word)\n",
        "          break\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        "\n",
        "print (' '.join([t for t in text if t]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R7Osy1kKGF5",
        "outputId": "309e4543-d578-4587-bce6-e75bde2646d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "today the Higher Labour Tribunal in Brasilia yesterday .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-tDCIQWJLmW",
        "outputId": "4e404a1a-c2cb-4575-b6a4-fc3885904140"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/176.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.23.5)\n",
            "Collecting boto3 (from pytorch-transformers)\n",
            "  Downloading boto3-1.33.11-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2023.6.3)\n",
            "Collecting sentencepiece (from pytorch-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses (from pytorch-transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2.1.0)\n",
            "Collecting botocore<1.34.0,>=1.33.11 (from boto3->pytorch-transformers)\n",
            "  Downloading botocore-1.33.11-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3->pytorch-transformers)\n",
            "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2023.11.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.0,>=1.33.11->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.11->boto3->pytorch-transformers) (1.16.0)\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.33.11 botocore-1.33.11 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.8.2 sacremoses-0.1.1 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car in the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the predicted next sub-word\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "# Print the predicted word\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCdvvrRJKUtA",
        "outputId": "5182cac6-c5e6-40d6-a22c-55f8063b1c56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1042301/1042301 [00:01<00:00, 961428.15B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 519229.19B/s]\n",
            "100%|██████████| 665/665 [00:00<00:00, 1014258.97B/s]\n",
            "100%|██████████| 548118077/548118077 [00:35<00:00, 15542167.10B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is the fastest car in the world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment -- Extend the code above to a sentence"
      ],
      "metadata": {
        "id": "FPDNGoo9OLis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car in the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    while True:\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "        # Get the predicted next sub-word\n",
        "        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "        predicted_token = tokenizer.decode([predicted_index])\n",
        "        if predicted_token == '<|endoftext|>' or '.' in predicted_token:\n",
        "            break\n",
        "        indexed_tokens += [predicted_index]\n",
        "        tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
        "\n",
        "# Decode the indexed tokens into a sentence\n",
        "predicted_text = tokenizer.decode(indexed_tokens)\n",
        "\n",
        "# Print the predicted sentence\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "id": "kK6GIh5NOPKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31108a0-8b07-456c-9c0c-87c53f27da75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is the fastest car in the world?\n",
            "\n",
            "The fastest car in the world is the Toyota Prius\n"
          ]
        }
      ]
    }
  ]
}
